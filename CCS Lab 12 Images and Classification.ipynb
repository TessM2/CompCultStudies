{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4/7TljM1CViFZXgW2E/Uh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Images and Classification**\n","\n","\n","Today, we're going to work on classificaiton of images using image embeddings. This is similar/analogous to classification of texts using word embeddings, but adapted to images.\n","\n","So, first we might want to be reminded: what are word embeddings? And how did they play into classificaton?\n","\n","What did clasification with language/words involve? What were the steps in the process? what decisions needed to be made?\n","\n","Review discussion....\n","\n","OK, now let's apply this to images. First, let's load in soem sample images.\n","\n","Remember we probably need to use GPUS!"],"metadata":{"id":"5PlIjAFpskmK"}},{"cell_type":"code","source":["!pip install -q datasets\n"],"metadata":{"id":"7XuqY_hyxM9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load dataset from Hugging Face\n","dataset = load_dataset(\"beans\")\n","\n","# Inspect structure\n","print(dataset)\n","print(dataset['train'][0])\n"],"metadata":{"id":"-mCT00sNxP-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def map_to_binary(example):\n","    example['label_binary'] = 1 if example['labels'] == 2 else 0  # 1 = healthy, 0 = diseased\n","    return example\n","\n","label_names = [\"diseased\", \"healthy\"]\n","\n"],"metadata":{"id":"93XN1VC2xVq-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply the corrected binary labels\n","binary_dataset = dataset.map(map_to_binary)\n","\n","# Show examples\n","show_images(binary_dataset['train'], label=1)  # healthy\n","show_images(binary_dataset['train'], label=0)  # diseased\n"],"metadata":{"id":"sTdGsX14RscI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So, we've got our images and labels sotred in binary_dataset. let's examine the structure of this for a second. what type of object is it? where/how is everything stored?"],"metadata":{"id":"xtPHOgfJyRta"}},{"cell_type":"code","source":["sample = binary_dataset['train'][0]\n","print(sample)\n","print(sample.keys())"],"metadata":{"id":"7swtxCizyRLS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each item in the dataset (binary_dataset['train']) is a dictionary with four keys:\n","\n","image_file_path\n","This is a string — the full file path to where the image is stored on your system (cached automatically when the dataset was downloaded). You won't usually need this, but it's there for internal traceability.\n","\n","image\n","This is a PIL.Image.Image object. It’s the actual image already loaded into memory and ready to be transformed or passed into a model. No need to manually open or read image files.\n","\n","labels\n","This is the original label from the dataset, an integer representing one of three classes:\n","\n","0 = angular_leaf_spot\n","\n","1 = bean_rust\n","\n","2 = healthy\n","\n","label_binary\n","This is a new label we created to simplify the task. It's also an integer:\n","\n","0 = diseased (meaning the original label was 0 or 1)\n","\n","1 = healthy (meaning the original label was 2)\n","\n"],"metadata":{"id":"SFLSj91_yyNZ"}},{"cell_type":"markdown","source":["**Embeddings**\n","\n","OK, let's make some embeddings. What are image embeddings? And what pretrained model should we use to make them?\n","\n","What are image embeddings?\n","\n","As we've discussed, an image embedding is a way of turning an image — which is just a big grid of pixel values — into a much smaller set of numbers that still capture the important information in the image. You can think of it like a summary: instead of using every single pixel, the embedding describes what kind of thing the image shows — textures, shapes, colors, or even abstract patterns — in a numerical format that a computer can work with.\n","\n","These embeddings come from deep neural networks that have already been trained on millions of images (remember the video we watched last week?). Instead of building our own network from scratch, we’ll reuse one of these powerful pretrained models to extract these embeddings for us.(Does anyone remember the pretrained models we used for word embeddings?)\n","\n","What tool are we using?\n","\n","To generate embeddings with a model, we’ll use a library (or, collection of prewritten code) called PyTorch, popular for deep learning and created by Meta (Facebook). PyTorch is especially useful for experimenting and learning because it’s written in a very intuitive, Pythonic way — it’s widely used in both research and teaching.\n","\n","There’s another popular library called TensorFlow, made by Google. TensorFlow is more common in industry settings, especially when building big production systems. It’s a bit more complex under the hood, but both libraries can be used to do similar things.\n"],"metadata":{"id":"01cO-3YMy0IQ"}},{"cell_type":"markdown","source":["Let's start by loading in the model we'll use for embeddings, Resnet-18"],"metadata":{"id":"EN5V40M9z7Pz"}},{"cell_type":"code","source":["# STEP 1: Import PyTorch and load a pretrained model\n","import torch\n","from torchvision import models, transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Load a pretrained ResNet-18 model\n","model = models.resnet18(pretrained=True)\n","model.eval()  # put it in inference mode (we’re not training)\n","\n","# Remove the last layer so we get the image embedding, not the final classification\n","embedding_model = torch.nn.Sequential(*list(model.children())[:-1])\n"],"metadata":{"id":"5VPxpr8Iz5mI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As is usual with model selection, we could have chosen others; this one is simple to use for embeddings tasks like this one because it's lightweight, fast, public, and easy to load in Python; but there's many you could use.\n","\n","Note also the step of removing the last \"layer\" of the model, to extract the embeddings\n","\n","When using a model like this on some data, we always have to know two things (at least); does anyone remember what they are?"],"metadata":{"id":"Yb4DXHU60NSv"}},{"cell_type":"code","source":["# We'll use one image from the dataset we've already loaded\n","sample = binary_dataset['train'][0]\n","img = sample['image']\n","\n","# Display the image\n","plt.imshow(img)\n","plt.axis(\"off\")\n","plt.title(\"Input Image\")\n","plt.show()\n"],"metadata":{"id":"Ne_awN37yx_i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We've here chosen an input image from our data. What's the format we're inputting the image in? (hint: note the key value from our dictionary we're selecting)"],"metadata":{"id":"-lqVeJgC04cM"}},{"cell_type":"markdown","source":["Before we can generate embeddings for the image we'll have to preprocess it:"],"metadata":{"id":"R0Wt3qxw1EMw"}},{"cell_type":"code","source":["# Define the standard preprocessing pipeline for ResNet\n","preprocess = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),  # Converts to PyTorch tensor and scales pixels to [0,1]\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # ImageNet mean\n","                         std=[0.229, 0.224, 0.225]),   # ImageNet std\n","])\n","\n","# Apply preprocessing\n","input_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension (1, 3, 224, 224)\n"],"metadata":{"id":"MiBM4RKj1Hmc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some notes on above:\n","\n","Neural networks need input in a specific format:\n","\n","Size: 224x224 pixels (center-cropped)\n","\n","Normalized pixel values (standard mean and std)\n","\n","On the resizing:\n","\n","First we resize to 256\n","This resizes the shorter side of the image to 256 pixels, while keeping the aspect ratio the same. The goal is to make sure the image is big enough that we can safely crop the center out.\n","\n","Then we Center Crop to 224x224\n","After resizing, we cut out the center 224×224 square of the image. This gives the final size the model expects.\n","\n","Many real images are not square — they may be tall, wide, etc.\n","\n","Resizing to 256 first makes sure we get enough image content; center cropping ensures consistency and avoids distortion (unlike directly resizing to 224x224, which would squash/stretch the images"],"metadata":{"id":"vGNlJc0QSFuU"}},{"cell_type":"markdown","source":["OK, what about the last line of that code? on batching and unsqueezing and so on?\n","\n","Neural networks in Pytorch will expect input in this shape:\n","(batch_size, num_channels, height, width)\n","\n","What this means is that the image is represented as a mutidimensional array or TENSOR that has these four dimensions:\n","\n","For a single image:\n","batch_size = 1 (we're processing one image at a time)\n","num_channels = 3 (RGB)\n","height = 224\n","width = 224\n","\n","The last line of the code above just adds the \"batch dimension\" of 1 to the beginning of our image data, foing from (3, 224, 224) to (1, 3, 224, 224)\n","\n","So, what does one of these tensors, with such dimensions, look like?\n","\n","Let's take a look at the \"input_tensor\" we just created"],"metadata":{"id":"5niVE3AkUqGT"}},{"cell_type":"code","source":["# Look at red channel of first image, top-left 3x3 region\n","print(input_tensor[0, 0, :3, :3])\n"],"metadata":{"id":"RibyRQHaULeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#or a larger slice:\n","\n","# Red channel (channel index 0), rows 50–60, cols 50–60\n","print(input_tensor[0, 0, 50:60, 50:60])\n"],"metadata":{"id":"NbruVFLGWQcT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OK so we've preprocessed our image and its now a tensor with 4 dimensions, the first being this necessary batch size of 1. Now let's turn it into embeddings (hint: that means turnng this whole buncha numbers into a whole buncha other numbers. yay!)"],"metadata":{"id":"vf0_P0BlWWu6"}},{"cell_type":"code","source":["# Make sure we're not tracking gradients (since we're not training)\n","with torch.no_grad():\n","    embedding = embedding_model(input_tensor)  # shape: [1, 512, 1, 1]\n"],"metadata":{"id":"1ZygVMmBWWLh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Embedding shape (raw):\", embedding.shape)\n"],"metadata":{"id":"6flZHqtLSFVl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model gives a 512-dimensional feature map, but it's still wrapped in 4D form. Let's flatten it:"],"metadata":{"id":"fd5ELin_ZBmZ"}},{"cell_type":"code","source":["embedding_vector = embedding.squeeze().numpy()\n","print(\"Embedding shape (flattened):\", embedding_vector.shape)\n","print(\"First 10 values:\", embedding_vector[:10])\n","\n"],"metadata":{"id":"mvEXHtRX02J1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training a classifier**\n","\n","first we'll have to make all of our images - the sick and healthy plants - into embeddings; we'll need those embeddings as a set of items with labels for the classifier, in the format we're familiar with from text classificaiton (two lists); and then we can proceed"],"metadata":{"id":"IUnxa1ZvaJ-o"}},{"cell_type":"code","source":["import random\n","import numpy as np\n","import torch\n","from collections import Counter\n","\n","# STEP 1: Select 50 from each class\n","diseased_indices = [i for i, ex in enumerate(binary_dataset['train']) if ex['label_binary'] == 0]\n","healthy_indices = [i for i, ex in enumerate(binary_dataset['train']) if ex['label_binary'] == 1]\n","\n","diseased_sample = random.sample(diseased_indices, 50)\n","healthy_sample = random.sample(healthy_indices, 50)\n","\n","# Combine them: first 50 diseased, then 50 healthy\n","balanced_indices = diseased_sample + healthy_sample\n","subset = binary_dataset['train'].select(balanced_indices)\n","\n","# STEP 2: Generate embeddings and labels\n","X = []\n","y = []\n","\n","for i, example in enumerate(subset):\n","    img = example['image']\n","    label = example['label_binary']  # 0 or 1\n","\n","    img_tensor = preprocess(img).unsqueeze(0)\n","\n","    with torch.no_grad():\n","        embedding = embedding_model(img_tensor)\n","        embedding_vector = embedding.squeeze().numpy()\n","\n","    X.append(embedding_vector)\n","    y.append(label)\n","\n","X = np.array(X)  # shape: (100, 512)\n","y = np.array(y)  # shape: (100,)\n","\n","print(\"Final label counts:\", Counter(y))  # Should show 50 of each\n","\n"],"metadata":{"id":"RL6CHHmEaXOU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OK, let's break down the code block above a bit; it brings together things we've learned about preparing items for classification with things we just learning about generating image embeddings."],"metadata":{"id":"IPz7zfdLafGN"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","\n","# STEP 1: Split data into training and testing sets\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","print(\"Training set size:\", X_train.shape)\n","print(\"Test set size:\", X_test.shape)\n","\n","\n","# STEP 2: Initialize and train the classifier\n","\n","clf = LogisticRegression(max_iter=1000)\n","clf.fit(X_train, y_train)\n","\n","# STEP 3: Make predictions and evaluate\n","\n","y_pred = clf.predict(X_test)\n","\n","# Show detailed classification metrics\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred, target_names=[\"Diseased\", \"Healthy\"]))\n"],"metadata":{"id":"P-ir98ZCcDN9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(note the lack of crossfold validation, like we did with the text; we could add it; here instead we just split into one train test split sample)"],"metadata":{"id":"azNBdt9ycIb1"}},{"cell_type":"markdown","source":["Let's check out running the classifier we just built on an image and see how it does"],"metadata":{"id":"eMZxykLAcYpS"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# STEP 1: Pick one image from the dataset (e.g., from the test split)\n","example = binary_dataset['train'][diseased_sample[0]]  # or use any index you like\n","img = example['image']\n","\n","# STEP 2: Display the image\n","plt.imshow(img)\n","plt.axis(\"off\")\n","plt.title(\"Image to Classify\")\n","plt.show()\n","\n","# STEP 3: Preprocess the image and get embedding\n","img_tensor = preprocess(img).unsqueeze(0)  # Shape: (1, 3, 224, 224)\n","\n","with torch.no_grad():\n","    embedding = embedding_model(img_tensor)\n","    embedding_vector = embedding.squeeze().numpy().reshape(1, -1)  # shape: (1, 512)\n","\n","# STEP 4: Use trained classifier to predict\n","pred_class = clf.predict(embedding_vector)[0]\n","pred_probs = clf.predict_proba(embedding_vector)[0]\n","\n","# STEP 5: Show result\n","class_names = [\"Diseased\", \"Healthy\"]\n","pred_label = class_names[pred_class]\n","confidence = pred_probs[pred_class] * 100\n","\n","print(f\"Predicted class: {pred_label}\")\n","print(f\"Confidence: {confidence:.2f}%\")\n"],"metadata":{"id":"1rM3ZPgXcciz"},"execution_count":null,"outputs":[]}]}