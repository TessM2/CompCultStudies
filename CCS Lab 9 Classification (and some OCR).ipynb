{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKC9JhinB0a0lkHbAkLVV5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","**Classification**\n","\n","Last week we talked about the process of classificaiton in basic outlines - taking two (or more, but let's start with two) groups of texts, labeled accordingly (type 1, type 2) in some quantity (1000 is a benchmark); selecting \"features\" for a classifier to use to distinguish them (e.g., in the simplest version, single words/word counts); selecting a type of classificaiton model (e.g. logistic reression, naive bayes); and then producing/analyzing an output that might be used in different ways.\n","\n","Oh, and we also mentioned the necessity of splitting up data into training and testing, or training testing and validation sets (or, as will be modeled below, using a slightly more complicated process, such as 10-fold cross validation, to prevent \"overfitting\"); and we considered, too, the ways in which we verify the functionality of a classification model (accuracy score) etc.\n","\n","Now let's put this all into practice through an example. We'll use texts by James Joyce - as in prior classes - and we'll use a program/package called \"scikitlearn\" to build/run classification models. You've seen this program before. Does anyone remember what we used scikitlearn for in the past?\n"],"metadata":{"id":"byy5LrJmy3yE"}},{"cell_type":"markdown","source":["First we need some texts. let's start by taking two joyce texts - dubliners and portrait - and spitting them up into mini chunks (let's say of 300 words each or so). Then let's see if a classifier can learn the difference betwen the chunks (i.e., classifying chunks from one text or the other with some accuracy). We'd expect this to work, even if our chunks are not super nicely divided (e.g., by paragraph). In a larger experiment, we could have the full books be single documents for classification, but then we'd need a lot more books!"],"metadata":{"id":"uWrFUvhM0CGY"}},{"cell_type":"code","source":["import requests\n","\n","# URLs to the texts\n","portrait_url = \"https://www.gutenberg.org/cache/epub/4217/pg4217.txt\"\n","dubliners_url = \"https://www.gutenberg.org/cache/epub/2814/pg2814.txt\"\n","\n","# Filenames to save\n","portrait_filename = \"portrait.txt\"\n","dubliners_filename = \"dubliners.txt\"\n","\n","# Download and save Portrait\n","response_portrait = requests.get(portrait_url)\n","with open(portrait_filename, \"w\", encoding=\"utf-8\") as f:\n","    f.write(response_portrait.text)\n","\n","# Download and save Dubliners\n","response_dubliners = requests.get(dubliners_url)\n","with open(dubliners_filename, \"w\", encoding=\"utf-8\") as f:\n","    f.write(response_dubliners.text)\n","\n","print(\"Texts downloaded and saved as 'portrait.txt' and 'dubliners.txt'\")\n"],"metadata":{"id":"Zkz93DYa0cbr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's clean out the gutenberg front and backmatter of each text by deleting everyhting before the first line and after the last."],"metadata":{"id":"F6wWS7Gr1R2r"}},{"cell_type":"markdown","source":["Find first line of portrait:"],"metadata":{"id":"91d7ejAK4b2l"}},{"cell_type":"code","source":["import re\n","\n","# Search for \"once upon a time\" in portrait_raw, case-insensitive\n","matches = list(re.finditer(r\"once upon a time\", portrait_raw, flags=re.IGNORECASE))\n","\n","# Print up to 3 matches with context\n","for i, match in enumerate(matches[:3]):\n","    start = max(0, match.start() - 100)\n","    end = match.end() + 100\n","    print(f\"\\n--- Match {i+1} ---\\n\")\n","    print(portrait_raw[start:end])\n"],"metadata":{"id":"6wSEde6N4OLD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Trim everything before"],"metadata":{"id":"F6BT2sLb4eYc"}},{"cell_type":"code","source":["# Step 1: Trim everything before \"Once upon a time\"\n","start_phrase = \"Once upon a time\"\n","start_idx = portrait_raw.lower().find(start_phrase.lower())\n","\n","if start_idx == -1:\n","    raise ValueError(\"Start phrase not found in Portrait.\")\n","\n","portrait_trimmed = portrait_raw[start_idx:]\n","\n","# Save intermediate result to inspect or continue cleaning later\n","with open(\"portrait_trimmed_start.txt\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(portrait_trimmed)\n","\n","print(\"Trimmed Portrait from start phrase. Saved as 'portrait_trimmed_start.txt'\")\n"],"metadata":{"id":"BqrrM7QB4fcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the trimmed file and preview beginning\n","with open(\"portrait_trimmed_start.txt\", \"r\", encoding=\"utf-8\") as f:\n","    portrait_trimmed = f.read()\n","\n","print(\"\\n--- PORTRAIT (START TRIMMED) BEGINNING ---\\n\")\n","print(portrait_trimmed[:1000])  # Print first 1000 characters\n"],"metadata":{"id":"FtHV9i8f4nkP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find last line of portrait:"],"metadata":{"id":"tMowHZW-4pzm"}},{"cell_type":"code","source":["import re\n","\n","# Define the full ending snippet, as it appears across lines\n","end_snippet_pattern = r\"old father.*?stand me now and ever in good\\s+stead\"\n","\n","# Use regex to match across line breaks in portrait_trimmed\n","match = re.search(end_snippet_pattern, portrait_trimmed, flags=re.IGNORECASE | re.DOTALL)\n","\n","if match:\n","    trimmed_portrait_final = portrait_trimmed[:match.end()]\n","\n","    # Save final cleaned version\n","    with open(\"portrait_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(trimmed_portrait_final)\n","\n","    print(\"âœ… Successfully matched and trimmed Portrait to correct ending.\")\n","else:\n","    print(\"âŒ Could not find ending snippet in portrait_trimmed.\")\n"],"metadata":{"id":"_6HZKF3I6erY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["check it worked"],"metadata":{"id":"OizJddrx6nt5"}},{"cell_type":"code","source":["# Load the final cleaned Portrait text\n","with open(\"portrait_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n","    portrait_clean = f.read()\n","\n","# Preview function\n","def preview_text(name, text, num_chars=500):\n","    print(f\"\\n--- {name} BEGINNING ---\\n\")\n","    print(text[:num_chars])\n","    print(f\"\\n--- {name} END ---\\n\")\n","    print(text[-num_chars:])\n","\n","# Check cleaned Portrait\n","preview_text(\"PORTRAIT (FINAL TRIM)\", portrait_clean)\n"],"metadata":{"id":"FSPfXmm76oo4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's clean dubliners in the same way:"],"metadata":{"id":"GHAu8OzE629I"}},{"cell_type":"code","source":["import re\n","\n","# Load full raw Dubliners text\n","with open(\"dubliners.txt\", \"r\", encoding=\"utf-8\") as f:\n","    dubliners_raw = f.read()\n","\n","# --- Trim start ---\n","start_snippet = \"There was no hope for him this time\"\n","start_idx = dubliners_raw.lower().find(start_snippet.lower())\n","\n","if start_idx == -1:\n","    raise ValueError(\"âŒ Start phrase not found in Dubliners.\")\n","else:\n","    dubliners_trimmed_start = dubliners_raw[start_idx:]\n","\n","# --- Trim end ---\n","# We'll use a fuzzy multiline match to capture the final sentence\n","end_pattern = (\n","    r\"his soul swooned slowly.*?the descent of their last end, upon all the living and the dead\"\n",")\n","\n","match = re.search(end_pattern, dubliners_trimmed_start, flags=re.IGNORECASE | re.DOTALL)\n","\n","if match:\n","    dubliners_clean = dubliners_trimmed_start[:match.end()]\n","\n","    # Save cleaned version\n","    with open(\"dubliners_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(dubliners_clean)\n","\n","    print(\"âœ… Successfully cleaned and saved 'dubliners_clean.txt'\")\n","else:\n","    raise ValueError(\"âŒ Could not find ending phrase in Dubliners.\")\n"],"metadata":{"id":"ygaRfwNF64xd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OK now let's split each text into a bunch of subdocuments, chunks of 300 words. we'll put the chunks in two separata dataframes, and give them index numbers to keep them in order"],"metadata":{"id":"H26N4zZJ7FDi"}},{"cell_type":"code","source":["import pandas as pd\n","\n","def chunk_text(text, chunk_size=300):\n","    # Tokenize text into words\n","    words = text.split()\n","    chunks = []\n","\n","    for i in range(0, len(words), chunk_size):\n","        chunk = \" \".join(words[i:i + chunk_size])\n","        chunks.append(chunk)\n","\n","    return chunks\n","\n","# Load the cleaned texts\n","with open(\"portrait_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n","    portrait_clean = f.read()\n","\n","with open(\"dubliners_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n","    dubliners_clean = f.read()\n","\n","# Chunk the texts\n","portrait_chunks = chunk_text(portrait_clean, chunk_size=300)\n","dubliners_chunks = chunk_text(dubliners_clean, chunk_size=300)\n","\n","# Create DataFrames with index numbers\n","portrait_df = pd.DataFrame({\n","    \"chunk_number\": range(len(portrait_chunks)),\n","    \"text\": portrait_chunks\n","})\n","\n","dubliners_df = pd.DataFrame({\n","    \"chunk_number\": range(len(dubliners_chunks)),\n","    \"text\": dubliners_chunks\n","})\n","\n","# Preview\n","print(\"âœ… Chunking complete. Portrait chunks:\", len(portrait_df))\n","print(\"âœ… Chunking complete. Dubliners chunks:\", len(dubliners_df))\n","\n","# Optional: save to CSVs for inspection\n","# portrait_df.to_csv(\"portrait_chunks.csv\", index=False)\n","# dubliners_df.to_csv(\"dubliners_chunks.csv\", index=False)\n"],"metadata":{"id":"XeDgRyGh7Mtm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cool. we'll notice that this isn't a huge amount of documents to work with to build a classifier. In an ideal world, we'd have 1000 of each. To achieve that we could make smaller chunks of about 100 words. But let's soldier on and see how this works."],"metadata":{"id":"e1Komluv7S-K"}},{"cell_type":"code","source":["portrait_df.head()"],"metadata":{"id":"2SjNHm4y7hpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dubliners_df.head()"],"metadata":{"id":"bM1fAhbL7lv0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, we need to know what format we need this data in to be used in scikitlearn's classifier.\n","\n","First, we need equal numbers of texts of each class. So, let's say we're going to take the first 225 chunks from each text. 225 chunks of dubliners and 225 chunks of portrait. what format do we need these in?\n","\n","to use the scikit learn classifier, we need this text formatted in this way:\n","\n","one big list of all the chunks: so, thats 225 dubliners chunks followed by 225 portrait chunks. and then, one big lists of all the labels for the texts. the labels basically should be 0 and 1, where 0 represents one class (portrait) and one represents another (dubliners)."],"metadata":{"id":"ahL0tYGJ7byN"}},{"cell_type":"code","source":["# Get the first 225 chunks from each (if not already sliced)\n","portrait_texts = portrait_chunks[:225]\n","dubliners_texts = dubliners_chunks[:225]\n","\n","# Combine into one list of features\n","X = portrait_texts + dubliners_texts\n","\n","# Create corresponding list of binary labels\n","# 0 for Portrait, 1 for Dubliners\n","y = [0] * len(portrait_texts) + [1] * len(dubliners_texts)\n","\n","# Sanity check\n","print(f\"âœ… Created feature list X with {len(X)} entries\")\n","print(f\"âœ… Created label list y with {len(y)} entries\")\n","print(f\"Example:\\nLabel: {y[0]} | Text: {X[0][:150]}...\")\n"],"metadata":{"id":"lCjS26pT7SlP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#first, out list of chunks, called X; let's check it out:\n","X"],"metadata":{"id":"i3a8O7HK84Zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#And let's check out ur list of labels, Y\n","y"],"metadata":{"id":"NO00cAlT6gfU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OK so, our data is now ready to be classifier\n","we still have to do a few things\n","first, we might need, in some cases, to split this data up into train and test sets\n","\n","instead, though, we're going to use the method that I suggest with classification which is using tenfold crossvalidation\n","\n","**pause for lecture on that"],"metadata":{"id":"WWJ1LU879jek"}},{"cell_type":"markdown","source":["Also, we need to select our features. Here, we're going to use the simple feature of single word counts. And we need to select what kind of classifier to use. We'll start with a logistic regression classifier. In a moment, we'll talk about what both these things mean (especially the secod) and how we can substitute them. BUT let's start by selecting them and moving forward\n","\n","The code below will train a logistic regression classification model on our data using bag of words features (single words/unigrams). It will use tenfold crossvlidation meaning training 10 diff models on different train and test set splits. BUT it will then SAVE the model with the highest accuracy score. So our saved output will be the best model."],"metadata":{"id":"jzay5y3y9wlb"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# ---------------------------------------------\n","# Setup: use 10-fold cross-validation manually\n","# ---------------------------------------------\n","\n","# We'll split the data into 10 folds\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","# Track scores and best model\n","fold_accuracies = []\n","best_accuracy = 0\n","best_model = None\n","\n","# ---------------------------------------------\n","# Loop through each fold\n","# ---------------------------------------------\n","for fold, (train_index, test_index) in enumerate(kf.split(X, y), 1):\n","    # Split the data into train and test sets for this fold\n","    X_train = [X[i] for i in train_index]\n","    X_test = [X[i] for i in test_index]\n","    y_train = [y[i] for i in train_index]\n","    y_test = [y[i] for i in test_index]\n","\n","    # -------------------------------\n","    # Build the pipeline\n","    # -------------------------------\n","    # CountVectorizer turns text into Bag-of-Words features (one column per word)\n","    # LogisticRegression assigns weights to each word to predict class (0 = Portrait, 1 = Dubliners)\n","    model = make_pipeline(\n","        CountVectorizer(),\n","        LogisticRegression(max_iter=1000)\n","    )\n","\n","    # Train the model on this fold's training set\n","    model.fit(X_train, y_train)\n","\n","    # Evaluate on this fold's test set\n","    y_pred = model.predict(X_test)\n","    acc = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(acc)\n","\n","    print(f\"Fold {fold} accuracy: {acc:.3f}\")\n","\n","    # Keep track of the best-performing model\n","    if acc > best_accuracy:\n","        best_accuracy = acc\n","        best_model = model  # Save the pipeline\n","\n","# ---------------------------------------------\n","# Report cross-validation results\n","# ---------------------------------------------\n","print(\"\\nâœ… 10-Fold Cross-Validation Summary:\")\n","print(f\"Accuracy scores: {np.round(fold_accuracies, 3)}\")\n","print(f\"Average accuracy: {np.mean(fold_accuracies):.3f}\")\n","print(f\"Best fold accuracy: {best_accuracy:.3f}\")\n"],"metadata":{"id":"UZa5wuQ2_kSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["woah, these are really high accuracy! the fact that some did perfectly might suggest \"overfitting\", an so we might want to investigate more by testing this out in different sets of data, in a real life experiment. Or, we might want to get rid of some input features that might make it too \"easy\" if were hoping for some sort of broad applicability to other texts from our classifier. However, the tenfold strategy should prevent overfitting, so we may be able to just trust that this works really well (as it should)."],"metadata":{"id":"XeBw1b_hAGVA"}},{"cell_type":"markdown","source":["But, now that we have our best model (or, one of them) Let's look at a few things we can do with this classifier we just built, before thinking about changing the chosen features or type."],"metadata":{"id":"Hi0-3vAK-lS2"}},{"cell_type":"markdown","source":["Fist, let's say we want to examine the features the classifier ended up using to make its distinctions. which words were most frequently associated with portrait (0)? which one with dubliners (1?)"],"metadata":{"id":"hTCAKamL-s4E"}},{"cell_type":"code","source":["# Get feature names and weights from the best model\n","vectorizer = best_model.named_steps['countvectorizer']\n","classifier = best_model.named_steps['logisticregression']\n","\n","feature_names = vectorizer.get_feature_names_out()\n","coefficients = classifier.coef_[0]\n","\n","# Get top words for each class\n","word_weights = list(zip(feature_names, coefficients))\n","top_portrait = sorted(word_weights, key=lambda x: x[1])[:20]\n","top_dubliners = sorted(word_weights, key=lambda x: x[1], reverse=True)[:20]\n","\n","print(\"\\nðŸŸ¦ Top 20 words for *Portrait* (class 0):\")\n","for word, weight in top_portrait:\n","    print(f\"{word:15} {weight:.4f}\")\n","\n","print(\"\\nðŸŸ¥ Top 20 words for *Dubliners* (class 1):\")\n","for word, weight in top_dubliners:\n","    print(f\"{word:15} {weight:.4f}\")\n"],"metadata":{"id":"G5GRtPsuAoBc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["well, that makes sense. we'd have guessed charafter names would be key to distinghsing the books. still, there are some other words that feel distinctive, too!"],"metadata":{"id":"4_Xq-OmNAujT"}},{"cell_type":"markdown","source":["OK, so we used a logistic regression classifier. what does this mean? well, this is one type of process the model can use to bild a classifier, as compared with some others called naive bayes, support vector machines. You can read about these in detail. But here's some short descriptions:"],"metadata":{"id":"IXVIWkLvBX-T"}},{"cell_type":"markdown","source":["]\n","Logistic Regression\n","\n","Logistic Regression is a linear classifier used for binary and multi-class classification.\n","It learns weights for each feature (e.g., word in a document) and uses them to calculate the probability that an input belongs to a class.\n","It's called \"logistic\" because it uses the logistic (sigmoid) function to squeeze predictions between 0 and 1.\n","Itâ€™s interpretable â€” you can see which words â€œpushâ€ the model toward a class.\n","\n","Naive Bayes\n","\n","Naive Bayes is a probabilistic classifier based on Bayes' Theorem.\n","It assumes that features (like words) are independent of one another â€” an assumption thatâ€™s rarely true, but works surprisingly well for text.\n","It calculates the probability of a class given the words in the document and chooses the most likely class.\n","Itâ€™s fast, simple, and often strong for bag-of-words models.\n","\n","Support Vector Machine (SVM)\n","\n","SVM is a margin-based classifier that tries to find the best boundary (hyperplane) between classes.\n","It looks for the decision boundary that maximizes the distance (margin) between the closest examples of each class.\n","Itâ€™s powerful in high-dimensional spaces like text, but less interpretable than logistic regression or Naive Bayes.\n","\n"],"metadata":{"id":"pI7CuHxnBmbE"}},{"cell_type":"markdown","source":["TLDR - log reg is probably the most conceptually simple and I like to start with it, but naive bayes is also commonly used. When in doubt, the proof is in the pudding. Use the type that gets you the highest accuracy scores"],"metadata":{"id":"v37haIoABauP"}},{"cell_type":"markdown","source":["Let's see what happens if we redo our classification model on dubliners vs portrait but use naive bayes instead"],"metadata":{"id":"YNVKqsheB1WH"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# ---------------------------------------------\n","# Setup: 10-fold cross-validation\n","# ---------------------------------------------\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","fold_accuracies = []\n","best_accuracy = 0\n","best_model = None\n","\n","# ---------------------------------------------\n","# Loop through folds\n","# ---------------------------------------------\n","for fold, (train_index, test_index) in enumerate(kf.split(X, y), 1):\n","    X_train = [X[i] for i in train_index]\n","    X_test = [X[i] for i in test_index]\n","    y_train = [y[i] for i in train_index]\n","    y_test = [y[i] for i in test_index]\n","\n","    # Build pipeline: CountVectorizer + Naive Bayes\n","    model = make_pipeline(\n","        CountVectorizer(),\n","        MultinomialNB()\n","    )\n","\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    acc = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(acc)\n","\n","    print(f\"Fold {fold} accuracy: {acc:.3f}\")\n","\n","    if acc > best_accuracy:\n","        best_accuracy = acc\n","        best_model = model\n","\n","# ---------------------------------------------\n","# Report results\n","# ---------------------------------------------\n","print(\"\\nâœ… 10-Fold Cross-Validation Summary (Naive Bayes):\")\n","print(f\"Accuracy scores: {np.round(fold_accuracies, 3)}\")\n","print(f\"Average accuracy: {np.mean(fold_accuracies):.3f}\")\n","print(f\"Best fold accuracy: {best_accuracy:.3f}\")\n"],"metadata":{"id":"bIUltwwFBXf-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again our most accurate was a 1.0, but there was only one. Let's compare the words used:"],"metadata":{"id":"CV8gD9zEB_L3"}},{"cell_type":"code","source":["# Extract the trained vectorizer and classifier from the best Naive Bayes pipeline\n","vectorizer = best_model.named_steps['countvectorizer']\n","classifier = best_model.named_steps['multinomialnb']\n","\n","# Get feature names (i.e., vocabulary) and log probabilities\n","feature_names = vectorizer.get_feature_names_out()\n","log_probs = classifier.feature_log_prob_  # shape: (n_classes, n_features)\n","\n","# Pair each word with its log-prob for each class\n","portrait_word_probs = list(zip(feature_names, log_probs[0]))\n","dubliners_word_probs = list(zip(feature_names, log_probs[1]))\n","\n","# Sort and get top 20 for each\n","top_portrait = sorted(portrait_word_probs, key=lambda x: x[1], reverse=True)[:20]\n","top_dubliners = sorted(dubliners_word_probs, key=lambda x: x[1], reverse=True)[:20]\n","\n","# Display results\n","print(\"\\nðŸŸ¦ Top 20 words strongly associated with *Portrait* (class 0):\\n\")\n","for word, logprob in top_portrait:\n","    print(f\"{word:15} {logprob:.4f}\")\n","\n","print(\"\\nðŸŸ¥ Top 20 words strongly associated with *Dubliners* (class 1):\\n\")\n","for word, logprob in top_dubliners:\n","    print(f\"{word:15} {logprob:.4f}\")\n"],"metadata":{"id":"mx2tB5KsCFYh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["wait a second - this is way different!!! The words for one thing are super generic. and the lists are the same. why is this?"],"metadata":{"id":"X85iyTMqCm_C"}},{"cell_type":"markdown","source":["Here's Why:\n","ðŸ’¡ Naive Bayes ranks words by how frequent they are in each class â€” independently\n","It doesn't compare word importance between classes.\n","\n","It just learns:\n","\n","\"In Portrait, these are the most common words\"\n","\"In Dubliners, these are the most common words\"\n","\n","So if the same high-frequency words appear a lot in both books (e.g., \"said\", \"father\", \"time\", etc.), theyâ€™ll rank high for both classes.\n","\n","âœ… It's frequency-based, not contrast-based.\n","\n","âš–ï¸ Logistic Regression, on the other hand:\n","Looks at which words help separate the classes.\n","\n","It says:\n","\n","â€œThis word pushes the model toward predicting Portrait.â€\n","â€œThat word pushes toward Dubliners.â€\n","\n","So you get different top words â€” because it's not about overall frequency, but discriminative power.\n","\n","âœ… It's contrast-based â€” it focuses on whatâ€™s unique about each class."],"metadata":{"id":"M7cOZtbTC1IB"}},{"cell_type":"markdown","source":["If we wanted to, we could use the naive bayes output to get the most distinctive words across the lists. But it would take an extra step!"],"metadata":{"id":"SEAKIMYnC8Vq"}},{"cell_type":"markdown","source":["OK, let's go back to logistic regression:"],"metadata":{"id":"jqbEA-uQDBwO"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# ---------------------------------------------\n","# Setup: 10-fold cross-validation\n","# ---------------------------------------------\n","kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","fold_accuracies = []\n","best_accuracy = 0\n","best_model_logreg = None  # store best-performing model\n","\n","# ---------------------------------------------\n","# Loop through folds\n","# ---------------------------------------------\n","for fold, (train_index, test_index) in enumerate(kf.split(X, y), 1):\n","    X_train = [X[i] for i in train_index]\n","    X_test = [X[i] for i in test_index]\n","    y_train = [y[i] for i in train_index]\n","    y_test = [y[i] for i in test_index]\n","\n","    # Build pipeline: CountVectorizer + LogisticRegression\n","    model = make_pipeline(\n","        CountVectorizer(),\n","        LogisticRegression(max_iter=1000)\n","    )\n","\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    acc = accuracy_score(y_test, y_pred)\n","    fold_accuracies.append(acc)\n","\n","    print(f\"Fold {fold} accuracy: {acc:.3f}\")\n","\n","    if acc > best_accuracy:\n","        best_accuracy = acc\n","        best_model_logreg = model\n","\n","# ---------------------------------------------\n","# Report results\n","# ---------------------------------------------\n","print(\"\\nâœ… 10-Fold Cross-Validation Summary (Logistic Regression):\")\n","print(f\"Accuracy scores: {np.round(fold_accuracies, 3)}\")\n","print(f\"Average accuracy: {np.mean(fold_accuracies):.3f}\")\n","print(f\"Best fold accuracy: {best_accuracy:.3f}\")\n"],"metadata":{"id":"k4MVjBQsDDcm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Another thing we can do with this model, now, is take a look at how \"well\" it classifies differnet chunks of the text into the categories. which can be interesting. Remember Ryan Cordell's comment, when he visited our class, about how one nice thing about how classiifers label genres is that they see them as \"probabilities\" vs just binary yes or no judgments. The classificaiton model won't just tell us a chunk of text is a 0 or 1 (portrait or dubliners). It will tell us its judged probability of the label."],"metadata":{"id":"ESWJWwBjDO_D"}},{"cell_type":"markdown","source":["Let's take a look at the first 20 chunks, each, from portrait and dubliners, and judge how likely they are, using our best performing logistic regression classifier, to be labeld as one or the other. In a way, it's like asking, which chunks of portrait are most \"portraity\" (relative to dubliners); and which chunks of dubliners are most \"dubliners-y\" (relative to portrait). This is what underwood and so mean by using classifiers to measure textual distance in a \"pespectival\" way. I.e., its almost like each txt gives us a perspective on the other."],"metadata":{"id":"koKahpQ7DwW1"}},{"cell_type":"code","source":["# Take the first 20 chunks from each text\n","test_chunks = portrait_chunks[:20] + dubliners_chunks[:20]\n","true_labels = [0]*20 + [1]*20  # 0 = Portrait, 1 = Dubliners\n","\n","# Use the best logistic regression model to predict probabilities\n","probs = best_model_logreg.predict_proba(test_chunks)\n","\n","# Display results\n","print(f\"{'Index':<5} {'True':<6} {'P(Portrait)':<15} {'P(Dubliners)':<15}\")\n","print(\"-\" * 45)\n","for i, (true_label, prob) in enumerate(zip(true_labels, probs)):\n","    p_portrait = prob[0]\n","    p_dubliners = prob[1]\n","    print(f\"{i:<5} {true_label:<6} {p_portrait:<15.3f} {p_dubliners:<15.3f}\")\n"],"metadata":{"id":"wO_kDRI3EHvO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's redo this, but examine the three highest and lowest probability of their class in each book"],"metadata":{"id":"cn_Y4HNXEfCf"}},{"cell_type":"code","source":["# First 20 chunks from each text\n","portrait_test_chunks = portrait_chunks[:20]\n","dubliners_test_chunks = dubliners_chunks[:20]\n","\n","# Get probabilities from the model\n","portrait_probs = best_model_logreg.predict_proba(portrait_test_chunks)\n","dubliners_probs = best_model_logreg.predict_proba(dubliners_test_chunks)\n","\n","# For Portrait: extract P(Portrait) = prob[:, 0]\n","portrait_scores = [(i, chunk, prob[0]) for i, (chunk, prob) in enumerate(zip(portrait_test_chunks, portrait_probs))]\n","portrait_sorted = sorted(portrait_scores, key=lambda x: x[2], reverse=True)\n","\n","# For Dubliners: extract P(Dubliners) = prob[:, 1]\n","dubliners_scores = [(i, chunk, prob[1]) for i, (chunk, prob) in enumerate(zip(dubliners_test_chunks, dubliners_probs))]\n","dubliners_sorted = sorted(dubliners_scores, key=lambda x: x[2], reverse=True)\n","\n","# Display Portrait results\n","print(\"\\nðŸŽ¨ Portrait chunks with HIGHEST model confidence (P=Portrait):\\n\")\n","for i, text, prob in portrait_sorted[:3]:\n","    print(f\"Chunk {i} | P(Portrait) = {prob:.3f}\\n{text[:300]}...\\n\")\n","\n","print(\"\\nðŸŽ¨ Portrait chunks with LOWEST model confidence (P=Portrait):\\n\")\n","for i, text, prob in portrait_sorted[-3:]:\n","    print(f\"Chunk {i} | P(Portrait) = {prob:.3f}\\n{text[:300]}...\\n\")\n","\n","# Display Dubliners results\n","print(\"\\nðŸ“˜ Dubliners chunks with HIGHEST model confidence (P=Dubliners):\\n\")\n","for i, text, prob in dubliners_sorted[:3]:\n","    print(f\"Chunk {i} | P(Dubliners) = {prob:.3f}\\n{text[:300]}...\\n\")\n","\n","print(\"\\nðŸ“˜ Dubliners chunks with LOWEST model confidence (P=Dubliners):\\n\")\n","for i, text, prob in dubliners_sorted[-3:]:\n","    print(f\"Chunk {i} | P(Dubliners) = {prob:.3f}\\n{text[:300]}...\\n\")\n"],"metadata":{"id":"0OyiAsqpArgR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What if we then used our classifier to classify trunks from a wholly differnet text? like, say, ulysses? let's say we took chunks from ulysses? would the model classify them as portrait or dubliners? it's an interesting question, because Ulysses has elements in common with both books. In early stages, e.g., it follows stephen daedalus closely, like portrait. later on, it moves more widely around dublin and its dubliners. however, stephen is present throughout.will the presence of his very name give a \"portrait\" signal to the whole text? or will it not? let's see..."],"metadata":{"id":"ipzfRXFyE4lZ"}},{"cell_type":"markdown","source":["Remember, we've got our model saved, as best_model_logreg\n","\n","so all we need is new text chunks to use it on"],"metadata":{"id":"D4JK1FT9GP-8"}},{"cell_type":"code","source":["import requests\n","\n","# Download the text of Ulysses\n","url = \"https://www.gutenberg.org/cache/epub/4300/pg4300.txt\"\n","response = requests.get(url)\n","\n","# Store the raw text\n","ulysses_raw = response.text\n","\n","# Preview the beginning\n","print(ulysses_raw[:1000])\n"],"metadata":{"id":"2LTLF-TOGV5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Update markers to match this specific version of the file\n","start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK ULYSSES ***\"\n","end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK ULYSSES ***\"\n","\n","# Find where the real book starts and ends\n","start_idx = ulysses_raw.find(start_marker) + len(start_marker)\n","end_idx = ulysses_raw.find(end_marker)\n","\n","# Extract and clean\n","ulysses_cleaned = ulysses_raw[start_idx:end_idx].strip()\n","\n","# Preview to confirm it worked\n","print(\"\\n--- Cleaned Ulysses Start ---\\n\")\n","print(ulysses_cleaned[:1000])\n"],"metadata":{"id":"r5BVZ5x9HMO_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's chunk the text"],"metadata":{"id":"XzeYkQMhHWaX"}},{"cell_type":"code","source":["# Function to chunk text by number of words\n","def chunk_text(text, chunk_size=300):\n","    words = text.split()\n","    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n","\n","# Chunk the cleaned Ulysses text\n","ulysses_chunks = chunk_text(ulysses_cleaned, chunk_size=300)\n","\n","# Preview: show number of chunks and the first one\n","print(f\"âœ… Total chunks created: {len(ulysses_chunks)}\")\n","print(\"\\n--- First Ulysses Chunk ---\\n\")\n","print(ulysses_chunks[0])\n"],"metadata":{"id":"2ygDTOqpHY9n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Step 1: Get predicted class labels and probabilities\n","ulysses_probs = best_model_logreg.predict_proba(ulysses_chunks)\n","ulysses_preds = best_model_logreg.predict(ulysses_chunks)\n","\n","# Step 2: Count number of chunks classified as each\n","num_portrait = sum(1 for label in ulysses_preds if label == 0)\n","num_dubliners = sum(1 for label in ulysses_preds if label == 1)\n","\n","print(\"ðŸ“Š Ulysses Classification Summary:\")\n","print(f\"Portrait-like chunks:  {num_portrait}\")\n","print(f\"Dubliners-like chunks: {num_dubliners}\")\n","print(f\"Total chunks:          {len(ulysses_preds)}\")\n","\n","# Step 3: Get P(Portrait) for each chunk\n","portrait_probs = [prob[0] for prob in ulysses_probs]\n","x_vals = list(range(len(portrait_probs)))\n","\n","# Step 4: Plot\n","plt.figure(figsize=(14, 5))\n","plt.plot(x_vals, portrait_probs, marker='o', linestyle='-', color='darkblue', label=\"P(Portrait)\")\n","plt.title(\"Stylistic Progression of *Ulysses*: Portrait-Like Probability per Chunk\")\n","plt.xlabel(\"Chunk Index (progress through book)\")\n","plt.ylabel(\"P(Portrait)\")\n","plt.ylim(0, 1)\n","plt.grid(True)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"q0uJutUXIeBQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["chunks 750-800 are \"giving\" strong portrait (lol), even though they are later in the text. what part of the text is this? maybe this is the \"circe\" episode where stephen shows up prominently again? or another late episode, like the ithaca where bloom and stephen are cavorting at night, since that's where stepehn shows up again toward the end of the text (despite bloom being the cnetral figure for much of the book's second half)? Ithaca is also highly academic in style, akin to portrait. so, if i had to guess, id guess that's what's being labeled as portrait like here. let's see:"],"metadata":{"id":"znWmwT3uI4sD"}},{"cell_type":"code","source":["# Define the range\n","start_idx = 750\n","end_idx = 800\n","\n","# Make sure we don't exceed the number of chunks\n","end_idx = min(end_idx, len(ulysses_chunks))\n","\n","print(f\"\\nðŸ” Examining Ulysses chunks {start_idx} to {end_idx}:\\n\")\n","\n","# Loop through the selected range\n","for i in range(start_idx, end_idx):\n","    chunk_text = ulysses_chunks[i]\n","    p_portrait = ulysses_probs[i][0]\n","    p_dubliners = ulysses_probs[i][1]\n","    predicted_label = \"Portrait\" if ulysses_preds[i] == 0 else \"Dubliners\"\n","\n","    print(f\"ðŸ“Œ Chunk {i}\")\n","    print(f\"Predicted: {predicted_label} | P(Portrait) = {p_portrait:.3f} | P(Dubliners) = {p_dubliners:.3f}\")\n","    print(f\"Text preview:\\n{chunk_text[:300]}...\\n{'-'*80}\\n\")\n"],"metadata":{"id":"ZdmbATHGJKYW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That's Ithaca! This makes a reassuring amunt of sense\n","That said, we don't learn too much by discovering that parts of ulysses with stephen daedalus are more likely to be labeled as \"portrait\" like. Maybe in a new experiment we might traun our classifier without the use of character names, to see if we can pull out more unexpected patterns of similarity?"],"metadata":{"id":"AitAGOIrJgV2"}},{"cell_type":"markdown","source":["**OCR**"],"metadata":{"id":"d-bHNhcRKDrs"}},{"cell_type":"markdown","source":["We may not have time for this in class, but I wanted to supply some sample code for those who want to use OCR, and with non English languages\n","\n","My understanding is that the program to use is tesseract, and it can do ocr on PDFs in many nonEnglish languages, including Georgian (Looking at you Megi).\n","\n","Let's test it out:"],"metadata":{"id":"m6ysXCUAKFvf"}},{"cell_type":"markdown","source":["First we need to load in tesseract, the ocr program, with support packages for the langauge we want. Here's how we'd load it in and the support package for georgian (with hashed out lines showing how we'd load in, say, turkish or russian)."],"metadata":{"id":"Wm28DiKaKy-Z"}},{"cell_type":"code","source":["# Install Tesseract and Georgian language support\n","!apt-get update\n","!apt-get install -y tesseract-ocr tesseract-ocr-kat\n"],"metadata":{"id":"eOeeuLQnI4eq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install Python OCR tools\n","!pip install pytesseract pdf2image pillow\n"],"metadata":{"id":"k668nuaHLaeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import in everything we just downloaded\n","\n","import pytesseract\n","from pdf2image import convert_from_path\n","from PIL import Image\n"],"metadata":{"id":"W63e9ZoHLgKL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["OK now we need a pdf! I'm going to take the sample georgian text from this link:\n","\n","https://www.language-museum.com/encyclopedia/g/georgian.php\n","\n","and make it a pdf and then load it in. You can do the same!\n","\n","\n"],"metadata":{"id":"khWgqXZkLlya"}},{"cell_type":"code","source":["from google.colab import files\n","\n","# Upload your Georgian PDF file (image-based, not digital text)\n","uploaded = files.upload()\n"],"metadata":{"id":"fAbq-iB8MgBe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In colab specifically before we can process tis pdf well also need a program called poppler"],"metadata":{"id":"jF66fdd6M8t_"}},{"cell_type":"code","source":["!apt-get install -y poppler-utils\n"],"metadata":{"id":"-yVJWE1ONAq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Convert all pages to images\n","pages = convert_from_path(pdf_path)\n"],"metadata":{"id":"pcHw9eMCNRI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Process each page\n","for i, page in enumerate(pages):\n","    text = pytesseract.image_to_string(page, lang='kat')  # 'kat' = Georgian\n","    print(f\"\\nðŸ“„ --- Page {i+1} ---\\n\")\n","    print(text[:1000])  # Print first 1000 characters of OCR output\n"],"metadata":{"id":"PyIoVdktNWf1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["cool."],"metadata":{"id":"8d0pl6PHNi1j"}},{"cell_type":"code","source":[],"metadata":{"id":"RWQXlkQINhTm"},"execution_count":null,"outputs":[]}]}